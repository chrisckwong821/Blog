---
layout: post
header:
title: Web Scraping - Football Match on Jockery Club
categories: [Python, Web Scraping]
tags: [Python, Web Scraping, Splinter]
fullview: true
comments: true
---

Recently I have been drawn to the technique of web-scraping. It is kind of a cool technique to collect first-hand data that is not readily available through an API or preporcessed. The good thing is, given your dataset is collected first-hand, your analysis may possible be unique and provoke interesting findings to yourself and the others. The downside is, you would have to spend additional time to collect, clean the data and design the amount and interval of data you would need. Personally, I see web-scraping as an essential technique that would enable me to have more flexibility in data analysis, do some preliminary anlysis before doing any real serious data collection for a particular problem.


As a toy problem, I have attempted to scrap the betting rates over the football matches on [Hong Kong Jockery Club](http://bet.hkjc.com/football/index.aspx?lang=en). I dont have the habit of watching football but just find that the data is quite straight-forward, and yet not available through any systematic channel like API. 


I ended up doing what I exactly wanted, in less than 30 lines of code, much fewer than I would have expected. The procedure is quite simple and applicable to other cases upon a little bit of adjustment. Basically, I used **Cron** and **Splinter**. 

Cron is a built-in utility in Mac which enables execution of scheduled tasks. So the concept is to schedule the code to be run at every interval to collect the data through a browser. In practice, the scraping should be done on a server to avoid interruption. But for a novice like me getting the program to run on my Mac would be my first step. 

Second, Splinter is a python layer of Selenium. It allows code-based browsing and information extraction. It has a high-level API to write automated test and scraping over websites. To anyone who has some expereience in Python this one is a no-brainer. But there must be some other abstraction layers of web browsing driver over other languages, at least the ones that I know are Javascript and R.

For anyone who wants to install Splinter, you need to install a driver first please type `brew install chromedriver` for chrome or `brew install geckodriver` for firefox. Then `pip install Splinter`. Then you are good to go!

Since I never used Cron, I decided to play around with this to understand how it works:
It turned out typing `crontab -e` would get me directly into an Vim editor. Holy shit that almost drove me crazy. I spent 15 minutes before I managed to exit....
Anyway, for testing purpose, I create a file called `123.sh` which just prints some string into a new file. Inside Crontab, I typed `* * * * * /bin/sh /path/to/my/file/123.sh`. 
`* * * * *` is the timer space each corresponds to minute, hour, day, month, and day in the week. Five-stars basically means execution every minute. So Cron would execute my `123.sh` every minute, and it did, nice!


Once the script is inside the Crontab. The script would be run automatically every minute
In addition, u can specify `MAILTO=@someemail` in order to send a mail to your system mail at `var/mail/users` for error log.

To execute my python script, within the crontab, my script is: `* * * * * /Library/Frameworks/Python.framework/Versions/3.6/bin/python3 /Users/pathtomyfile` The first path links to the python which I want to invoke, and the second path points to my python script.

Lastly, type `which python` to check where your python locate, for me, it is the previous path, so you can replace it with your own path.

Once the Crontab is up and running, I have turned my attention to the crawler itself.
Before crawling, I have taken a look over the website first.


![png]({{ site.baseurl }}/assets/media/Splinter/Picture1.png)


As you can see, the website already presents the data in a clean table, including match number, match date, team names and odds. 

On top of these data, I decided to include the refresh time, and current date for sorting later. Now, Let's open an editor and put up a few lines:



```python
from splinter import Browser
browser = Browser('chrome')
browser.visit('http://bet.hkjc.com/football/default.aspx')
```

These three lines would open a chrome browser and visit the stated website.

Now, because I have already figured out what I want, so I go straight to inspect their ids. 
I right click on the browser, click **"Inspect"**. Then I inspected each element on the html including their class, id, css and xpath.

I went to the table and find that the id of each row is formated as `rmid110876` and `rmid110877` and so on, so I decided to to use regular expression to extract their ids and loop through each of its id.



```python

import re
pattern = '(rmid11[0-9]{4})'
ids = re.findall(pattern, browser.html)

```

This block shoud return a list of ids in the website. Unfortunately it returned `None`. After some inspection I found that the entire table is embedded on a frame, which actually links the website to another url. Maybe they don't want other to crawl the betting odds I guess? Anyway I replaced the previous url with the new one.


```python

browser.visit('http://bet.hkjc.com/football/index.aspx?lang=en')

```

Now the `ids` indeeds contains a list of ids obtained from the website.
Next, I need to crawl the text of each elment, format it nicely:
Since the element contains a text attribute which is a string like this: `'THU 3 Orebro vs AFC Eskilstuna 22/09 00:00\n1.48 4.00 5.10'`, so I simply separate the information I want into groups.


```python
matchtime, home, away = [], [] , []
odd_home, odd_draw, odd_away = [], [], []

for match in ids:
    entry = browser.find_by_id(match).text
    regex = re.search(r'[\d]\s(.+)\svs\s(\D+)\s(.+)\n(\S+)\s(\S+)\s(\S+)', entry)
    home.append(regex.group(1))
    away.append(regex.group(2))
    matchtime.append(regex.group(3))
    odd_home.append(regex.group(4))
    odd_draw.append(regex.group(5))
    odd_away.append(regex.group(6))
    
#get the latest refresh time 
refresh = [browser.find_by_id('sRefreshTime').value] * len(ids)

```

The code may look repetitive and I am sure there would be some ways to shorten its length, but its simplicity in logic makes the export part as simply as a csv append:



```python

import csv
import time

#so to append the current date on each row for sorting later
date = [time.strftime("%Y/%m/%d")] * len(ids)

#zip the eight columns into rows
rows = zip(matchtime,date,refresh,home,away,odd_home,odd_draw,odd_away)

with open('football.odds.csv','a') as f:
    writer = csv.writer(f)
    for row in rows:
        writer.writerow(row)
    f.close()

browser.quit()

```

There you go. Put the code inside a script and invoke it from Crontab as mentioned. You would be bombarded with data every minute. My csv file looks like this:

<script src="https://gist.github.com/chrisckwong821/eff4adac9d386b64cb7a5b18b50a95af.js"></script>


To stop Crontab, you can `contab file.txt` to copy your content to a backup file, then erase the content by `contab -r`. 

This is only an experiment, crawaling of larger scale would have consideration in latency, set-up of remote server and potential rejection of client due to frequent access.

For next step, I would probably test the code on a remote server or virtual machine, just to see if any new problems would arise. 
